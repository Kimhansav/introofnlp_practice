{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kimhansav/introofnlp_practice/blob/main/12_05(BiLSTM%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%EA%B0%9C%EC%B2%B4%EB%AA%85_%EC%9D%B8%EC%8B%9D(Named_Entity_Recognition%2C_NER))practice_%2B_12_06(BiLSTM_CRF%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_%EA%B0%9C%EC%B2%B4%EB%AA%85_%EC%9D%B8%EC%8B%9D)practice_%2B_12_07(%EB%AC%B8%EC%9E%90_%EC%9E%84%EB%B2%A0%EB%94%A9(Character_Embedding)_%ED%99%9C%EC%9A%A9%ED%95%98%EA%B8%B0)practice_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtqwlT8XQ92d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/12.%20Sequence%20Labeling/dataset/ner_dataset.csv\", filename=\"ner_dataset.csv\")\n",
        "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "4WHaRBwioqMj",
        "outputId": "69f96f42-e3b7-4675-dcb1-688661973e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "HTTP Error 404: Not Found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0f567c0682f7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/12.%20Sequence%20Labeling/dataset/ner_dataset.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ner_dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ner_dataset.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_splittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[:5]"
      ],
      "metadata": {
        "id": "effOsJW7or1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('데이터프레임 행의 개수 : {}'.format(len(data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "d5S6hajUos6d",
        "outputId": "447f82fc-c2c3-4388-da6d-e218a80c3c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c2a0165f8ca1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'데이터프레임 행의 개수 : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('데이터에 null 값이 있는지 유무 : ' + str(data.isnull().values.any()))"
      ],
      "metadata": {
        "id": "R26RRKMGpNjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('어떤 열에 Null값이 있는지 출력')\n",
        "print('==============================')\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "oXQQhsMBpdZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('sentence # 열의 중복을 제거한 값의 개수 : {}'.format(data['Sentence #'].nunique()))\n",
        "print('Word 열의 중복을 제거한 값의 개수 : {}'.format(data.Word.nunique()))\n",
        "print('Tag 열의 중복을 제거한 값의 개수 : {}'.format(data.Tag.nunique()))"
      ],
      "metadata": {
        "id": "G1xUNTyRplO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tag 열의 각각의 값의 개수 카운트')\n",
        "print('================================')\n",
        "print(data.groupby('Tag').size().reset_index(name = 'count'))"
      ],
      "metadata": {
        "id": "Je6qy0I_p8rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.fillna(method = 'ffill')\n",
        "print(data.tail())"
      ],
      "metadata": {
        "id": "Z4HOTRF7qvk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('데이터에 Null 값이 있는지 유무 : ' + str(data.isnull().values.any()))"
      ],
      "metadata": {
        "id": "UArgZonLq2P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Word'] = data['Word'].str.lower()\n",
        "print('Word 열의 중복을 제거한 값의 개수 : {}'.format(data.Word.nunique()))"
      ],
      "metadata": {
        "id": "nbC__FL8rBE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[:5])"
      ],
      "metadata": {
        "id": "L1jLm7BUrOSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func = lambda temp: [(w, t) for w, t in zip(temp[\"Word\"].values.tolist(), temp[\"Tag\"].values.tolist())]\n",
        "tagged_sentences = [t for t in data.groupby(\"Sentence #\").apply(func)]\n",
        "print(\"전체 샘플 개수: {}\".format(len(tagged_sentences)))"
      ],
      "metadata": {
        "id": "p17D1jCJrRGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tagged_sentences[0])"
      ],
      "metadata": {
        "id": "rzHLg9KKr_0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences, ner_tags = [], []\n",
        "for tagged_sentence in tagged_sentences: #47959개의 문장 샘플을 1개씩 출력\n",
        "  #각 샘플에서 단어들은 sentence에, 개체명 태깅정보는 tag_info에 저장\n",
        "  sentence, tag_info = zip(*tagged_sentence)\n",
        "  sentences.append(list(sentence))\n",
        "  ner_tags.append(list(tag_info))"
      ],
      "metadata": {
        "id": "FETHGOUnsCSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[0])\n",
        "print(ner_tags[0])"
      ],
      "metadata": {
        "id": "5NxtLe1qtWEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[98])\n",
        "print(ner_tags[98])"
      ],
      "metadata": {
        "id": "WnWLRnsdtdsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('샘플의 최대 길이 : %d' % max(len(l) for l in sentences))\n",
        "print('샘플의 평균 길이 : %d' % (sum(map(len, sentences)) / len(sentences)))\n",
        "plt.hist([len(s) for s in sentences], bins = 50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lrtjtv6btp0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모든 단어를 사영하며 인덱스 1에는 단어 'OOV'를 할당.\n",
        "src_tokenizer = Tokenizer(oov_token = 'OOV')\n",
        "#태깅 정보들은 내부적으로 대문자를 유지한 채 저장\n",
        "tar_tokenizer = Tokenizer(lower = False)\n",
        "\n",
        "src_tokenizer.fit_on_texts(sentences)\n",
        "tar_tokenizer.fit_on_texts(ner_tags)"
      ],
      "metadata": {
        "id": "x_TX9B5Yt_vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(src_tokenizer.word_index) + 1\n",
        "tag_size = len(tar_tokenizer.word_index) + 1\n",
        "print('단어 집합의 크기 : {}'.format(vocab_size))\n",
        "print('개체명 태깅 정보 집합의 크기 : {}'.format(tag_size))"
      ],
      "metadata": {
        "id": "uU6S90tCudIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 OOV의 인덱스 : {}'.format(src_tokenizer.word_index['OOV']))"
      ],
      "metadata": {
        "id": "euF0qfR_urQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_data = src_tokenizer.texts_to_sequences(sentences)\n",
        "y_data = tar_tokenizer.texts_to_sequences(ner_tags)"
      ],
      "metadata": {
        "id": "-feArUx9u5NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "id": "sTClCof9vAk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = src_tokenizer.word_index\n",
        "index_to_word = src_tokenizer.index_word\n",
        "ner_to_index = tar_tokenizer.word_index\n",
        "index_to_ner = tar_tokenizer.index_word\n",
        "index_to_ner[0] = 'PAD'\n",
        "\n",
        "print(index_to_ner)"
      ],
      "metadata": {
        "id": "G1ID1Y9PvGMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded = []\n",
        "for index in X_data[0]: #첫번째 샘플 안의 인덱스들에 대해서\n",
        "  decoded.append(index_to_word[index])\n",
        "\n",
        "print('기존의 문장 : {}'.format(sentences[0]))\n",
        "print('디코딩 문장 : {}'.format(decoded))"
      ],
      "metadata": {
        "id": "H0VIG2AUvXkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 70\n",
        "X_data = pad_sequences(X_data, padding = 'post', maxlen = max_len)\n",
        "y_data = pad_sequences(y_data, padding = 'post', maxlen = max_len)"
      ],
      "metadata": {
        "id": "uQ0mEQOevqHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train_int, y_test_int = train_test_split(X_data, y_data, test_size = .2, random_state = 777)"
      ],
      "metadata": {
        "id": "bOI207IRv63W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train_int, num_classes = tag_size)\n",
        "y_test = to_categorical(y_test_int, num_classes = tag_size)"
      ],
      "metadata": {
        "id": "Y45mknG5wFlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\n",
        "print('훈련 샘플 레이블(정수 인코딩)의 크기 : {}'.format(y_train_int.shape))\n",
        "print('훈련 샘플 레이블(원-핫 인코딩)의 크기 : {}'.format(y_train.shape))\n",
        "print('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\n",
        "print('테스트 샘플 레이블(정수 인코딩)의 크기 : {}'.format(y_test_int.shape))\n",
        "print('테스트 샘플 레이블(원-핫 인코딩)의 크기 : {}'.format(y_test.shape))"
      ],
      "metadata": {
        "id": "NdUifN8NwRnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "embedding_dim = 128\n",
        "hidden_units = 256\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, mask_zero = True))\n",
        "model.add(Bidirectional(LSTM(hidden_units, return_sequences = True)))\n",
        "model.add(TimeDistributed(Dense(tag_size, activation=('softmax'))))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=128, epochs=6, validation_split=0.1)"
      ],
      "metadata": {
        "id": "x2DriBIpwXDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.\n",
        "y_predicted = model.predict(np.array([X_test[i]])) # 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
        "y_predicted = np.argmax(y_predicted, axis=-1) # 확률 벡터를 정수 인코딩으로 변경함.\n",
        "labels = np.argmax(y_test[i], -1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경함.\n",
        "\n",
        "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
        "print(35 * \"-\")\n",
        "\n",
        "for word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\n",
        "    if word != 0: # PAD값은 제외함.\n",
        "        print(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_ner[tag], index_to_ner[pred]))"
      ],
      "metadata": {
        "id": "qOba9E_LxD8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#F1-score"
      ],
      "metadata": {
        "id": "3Zh0-xqlxKWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['B-PER', 'I-PER', 'O', 'O', 'B-MISC', 'O','O','O','O','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O','O','B-MISC','I-MISC','I-MISC','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O']\n",
        "predicted = ['O'] * len(labels)\n",
        "print('예측값 :',predicted)"
      ],
      "metadata": {
        "id": "tbdN7n8Ox2sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hit = 0 # 정답 개수\n",
        "for tag, pred in zip(labels, predicted):\n",
        "    if tag == pred:\n",
        "        hit +=1 # 정답인 경우에만 +1\n",
        "accuracy = hit/len(labels) # 정답 개수를 총 개수로 나눈다.\n",
        "print(\"정확도: {:.1%}\".format(accuracy))"
      ],
      "metadata": {
        "id": "4bRnshnnx7aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install seqeval\n",
        "#import seqeval"
      ],
      "metadata": {
        "id": "qxZ1OKExyK8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "print(classification_report([labels], [predicted]))"
      ],
      "metadata": {
        "id": "6KHJ5aXOyNNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['B-PER', 'I-PER', 'O', 'O', 'B-MISC', 'O','O','O','O','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O','O','B-MISC','I-MISC','I-MISC','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O']\n",
        "predicted = ['B-PER', 'I-PER', 'O', 'O', 'B-MISC', 'O','O','O','O','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O']\n",
        "\n",
        "print(classification_report([labels], [predicted]))"
      ],
      "metadata": {
        "id": "iaCEcG8MyfyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import f1_score, classification_report\n",
        "\n",
        "def sequences_to_tag(sequences):\n",
        "  result = []\n",
        "  #전체 시퀀스로부터 시퀀스를 하나씩 꺼낸다\n",
        "  for sequence in sequences:\n",
        "    word_sequence = []\n",
        "    #시퀀스로부터 확률 벡터 또는 원-핫 벡터를 하나씩 꺼낸다\n",
        "    for pred in sequence:\n",
        "      #정수로 변환. 예를 들어 pred가 [0, 0, 1, 0, 0]이라면 1의 인덱스인 2를 리턴한다\n",
        "      pred_index = np.argmax(pred)\n",
        "      #index_to_ner을 사용하여 정수를 태깅 정보로 변환, 'PAD'는 'O'로 변경\n",
        "      word_sequence.append(index_to_ner[pred_index].replace(\"PAD\", \"O\"))\n",
        "    result.append(word_sequence)\n",
        "  return result\n",
        "\n",
        "y_predicted = model.predict([X_test])\n",
        "pred_tags = sequences_to_tag(y_predicted)\n",
        "test_tags = sequences_to_tag(y_test)\n",
        "\n",
        "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n",
        "print(classification_report(test_tags, pred_tags))"
      ],
      "metadata": {
        "id": "oZWBxfwUyrEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12-06(BiLSTM-CRF를 이용한 개체명 인식)\n"
      ],
      "metadata": {
        "id": "TTk4RuX14LUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-crf"
      ],
      "metadata": {
        "id": "jGkuKa8c3JBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Input, Bidirectional, TimeDistributed, Embedding, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras_crf import CRFModel\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "\n",
        "embedding_dim = 128\n",
        "hidden_units = 64\n",
        "dropout_ratio = 0.3\n",
        "\n",
        "sequence_input = Input(shape = (max_len,), dtype = tf.int32, name = 'sequence_input')\n",
        "\n",
        "model_embedding = Embedding(input_dim = vocab_size, output_dim = embedding_dim, input_length = max_len)(sequence_input)\n",
        "\n",
        "model_bilstm = Bidirectional(LSTM(units=hidden_units, return_sequences=True))(model_embedding)\n",
        "\n",
        "model_dropout = TimeDistributed(Dropout(dropout_ratio))(model_bilstm)\n",
        "\n",
        "model_dense = TimeDistributed(Dense(tag_size, activation='relu'))(model_dropout)\n",
        "\n",
        "base = Model(inputs = sequence_input, outputs = model_dense)\n",
        "model = CRFModel(base, tag_size)\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.001), metrics = 'accuracy')"
      ],
      "metadata": {
        "id": "xesik2L-367g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('bilstm_crf/cp.ckpt', monitor='val_decode_sequence_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "history = model.fit(X_train, y_train_int, batch_size=128, epochs=15, validation_split=0.1, callbacks=[mc, es])"
      ],
      "metadata": {
        "id": "oOLFx_kr5p9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('bilstm_crf/cp.ckpt')\n",
        "\n",
        "i = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.\n",
        "y_predicted = model.predict(np.array([X_test[i]]))[0] # 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
        "labels = np.argmax(y_test[i], -1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경.\n",
        "\n",
        "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
        "print(35 * \"-\")\n",
        "\n",
        "for word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\n",
        "    if word != 0: # PAD값은 제외함.\n",
        "        print(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_ner[tag], index_to_ner[pred]))"
      ],
      "metadata": {
        "id": "MkpQDf1V5q_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted = model.predict(X_test)[0]"
      ],
      "metadata": {
        "id": "j4nknwjF7CYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_predicted[:2])"
      ],
      "metadata": {
        "id": "aP8qinKF7FRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sequences_to_tag_for_crf(sequences):\n",
        "  result = []\n",
        "  #전체 시퀀스로부터 시퀀스를 하나씩 꺼낸다\n",
        "  for sequence in sequences:\n",
        "    word_sequence = []\n",
        "    #시퀀스로부터 예측 정수 레이블을 하나씩 꺼낸다\n",
        "    for pred_index in sequence:\n",
        "      #index_to_ner을 사용하여 정수를 태깅 정보로 변환. 'PAD'는 'O'로 변경\n",
        "      word_sequence.append(index_to_ner[pred_index].replace(\"PAD\", \"O\"))\n",
        "    result.append(word_sequence)\n",
        "  return result\n",
        "\n",
        "pred_tags = sequences_to_tag_for_crf(y_predicted)\n",
        "test_tags = sequences_to_tag(y_test)\n",
        "\n",
        "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n",
        "print(classification_report(test_tags, pred_tags))"
      ],
      "metadata": {
        "id": "-o34895e7QxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12-07(문자 임베딩(Character Embedding) 활용하기)practice"
      ],
      "metadata": {
        "id": "bk6VigJe8rF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#char_vocab 만들기\n",
        "words = list(set(data[\"Word\"].values))\n",
        "chars = set([w_i for w in words for w_i in w])\n",
        "chars = sorted(list(chars))\n",
        "print('문자 집합 : ',chars)"
      ],
      "metadata": {
        "id": "5T56RRPi8sTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index = {c: i + 2 for i, c in enumerate(chars)}\n",
        "char_to_index[\"OOV\"] = 1\n",
        "char_to_index[\"PAD\"] = 0\n",
        "\n",
        "index_to_char = {}\n",
        "for key, value in char_to_index.items():\n",
        "  index_to_char[value] = key"
      ],
      "metadata": {
        "id": "Mrtl-Tu2R_YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_char = 15\n",
        "\n",
        "#문자 시퀀스에 대해 패딩하는 함수\n",
        "def padding_char_indice(char_indice, max_len_char):\n",
        "  return pad_sequences(char_indice, maxlen = max_len_char, padding = 'post', value = 0)\n",
        "\n",
        "#각 단어를 문자 시퀀스로 변환 후 패딩 진행\n",
        "def integer_coding(sentences):\n",
        "  char_data = []\n",
        "  for ts in sentences:\n",
        "    word_indice = [word_to_index[t] for t in ts]\n",
        "    char_indice = [[char_to_index[char] for char in t] for t in ts]\n",
        "    char_indice = padding_char_indice(char_indice, max_len_char)\n",
        "  return char_data\n",
        "\n",
        "#문자 단위 정수 인코딩 결과\n",
        "X_char_data = integer_coding(sentences)"
      ],
      "metadata": {
        "id": "L1YdZB5bS8FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#정수 인코딩 이전의 기존 문장\n",
        "print('기존 문장 :', sentences[0])"
      ],
      "metadata": {
        "id": "Zprckat9V1Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 단위 정수 인코딩 + 패딩\n",
        "print('단어 단위 정수 인코딩 :')\n",
        "print(X_data[0])"
      ],
      "metadata": {
        "id": "uw2LSr7YV7RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자 단위 정수 인코딩\n",
        "print('문자 단위 정수 인코딩 :')\n",
        "print(X_char_data[0])"
      ],
      "metadata": {
        "id": "ellKqxIOWEdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_char_data = pad_sequences(X_char_data, maxlen = max_len, padding = 'post', value = 0)"
      ],
      "metadata": {
        "id": "0FKuGoeoWLyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_char_train, X_char_test, _, _ = train_test_split(X_char_data, y_data, test_size = .2, random_state = 777)\n",
        "\n",
        "X_char_train = np.array(X_char_train)\n",
        "X_char_test = np.array(X_char_test)"
      ],
      "metadata": {
        "id": "kt2JGz9JWaFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0])"
      ],
      "metadata": {
        "id": "32vWoIzYWrSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(index_to_word[150])"
      ],
      "metadata": {
        "id": "HSHXye7IWuJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(' '.join([index_to_char[index] for index in X_char_train[0][0]])) #3중 인덱스 : [문장][단어][문자]"
      ],
      "metadata": {
        "id": "RH943VNbW1hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\n",
        "print('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\n",
        "print('훈련 샘플 char 데이터의 크기 : {}'.format(X_char_train.shape))\n",
        "print('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\n",
        "print('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))"
      ],
      "metadata": {
        "id": "MzyzU7UUXOL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Input, TimeDistributed, Dropout, concatenate, Bidirectional, LSTM, Conv1D, Dense, MaxPooling1D, Flatten\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.initializers import RandomUniform\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "from keras_crf import CRFModel\n",
        "\n",
        "embedding_dim = 128\n",
        "char_embedding_dim = 64\n",
        "dropout_ratio = 0.5\n",
        "hidden_units = 256\n",
        "num_filters = 30\n",
        "kernel_size = 3\n",
        "\n",
        "#단어 임베딩\n",
        "word_ids = Input(shape = (None,),dtype = 'int32', name = 'words_input')\n",
        "word_embeddings = Embedding(input_dim = vocab_size, output_dim = embedding_dim)(word_ids)\n",
        "\n",
        "#char 임베딩\n",
        "char_ids = Input(shape = (None, max_len_char,),dtype = 'int32', name = 'words_input')\n",
        "embed_char_out = TimeDistributed(Embedding(len(char_to_index), char_embedding_dim, embeddings_initializer = RandomUniform(minval = -0.5, maxval = 0.5)), name = 'char_embedding')(char_ids)\n",
        "dropout = Dropout(dropout_ratio)(embed_char_out)\n",
        "\n",
        "#char 임베딩에 대해서는 Conv1D 수행\n",
        "conv1d_out = TimeDistributed(Conv1D(kernel_size = kernel_size, filters = num_filters, padding = 'same', activation = 'tanh', strides = 1))(dropout)\n",
        "maxpool_out = TimeDistributed(MaxPooling1D(max_len_char))(conv1d_out)\n",
        "char_embeddings = TimeDistributed(Flatten())(maxpool_out)\n",
        "char_embeddings = Dropout(dropout_ratio)(char_embeddings)\n",
        "\n",
        "#char 임베딩을 Conv1D 수행한 뒤에 단어 임베딩과 연결\n",
        "output = concatenate([word_embeddings, char_embeddings])\n",
        "\n",
        "#연결한 벡터를 가지고 문장의 길이만큼 LSTM을 수행\n",
        "output = Bidirectional(LSTM(hidden_units, return_sequences = True, dropout = dropout_ratio))(output)\n",
        "\n",
        "#출력층\n",
        "output = TimeDistributed(Dense(tag_size, activation = 'softmax'))(output)\n",
        "\n",
        "model = Model(inputs = [word_ids, char_ids], outputs = [output])\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'nadam', metrics = ['acc'])"
      ],
      "metadata": {
        "id": "0ss8LP-0XOto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('bilstm_cnn.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "history = model.fit([X_train, X_char_train], y_train, batch_size=128, epochs=15, validation_split=0.1, verbose=1, callbacks=[es, mc])"
      ],
      "metadata": {
        "id": "WpiD8xh7a3c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('bilstm_cnn.h5')\n",
        "\n",
        "i = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.\n",
        "# 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
        "y_predicted = model.predict([np.array([X_test[i]]), np.array([X_char_test[i]])])\n",
        "\n",
        "y_predicted = np.argmax(y_predicted, axis=-1) # 확률 벡터를 정수 인코딩으로 변경.\n",
        "labels = np.argmax(y_test[i], -1) # 원-핫 인코딩을 정수 인코딩으로 변경.\n",
        "\n",
        "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
        "print(35 * \"-\")\n",
        "\n",
        "for word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\n",
        "    if word != 0: # PAD값은 제외함.\n",
        "        print(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_ner[tag], index_to_ner[pred]))"
      ],
      "metadata": {
        "id": "mHbZ0rtba-uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted = model.predict([X_test, X_char_test])\n",
        "pred_tags = sequences_to_tag(y_predicted)\n",
        "test_tags = sequences_to_tag(y_test)\n",
        "\n",
        "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n",
        "print(classification_report(test_tags, pred_tags))"
      ],
      "metadata": {
        "id": "0hBCYVsRbTq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "문자임베딩(1D CNN) + CRF"
      ],
      "metadata": {
        "id": "EShlCIvMbv3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "char_embedding_dim = 64\n",
        "dropout_ratio = 0.5\n",
        "hidden_units = 256\n",
        "num_filters = 30\n",
        "kernel_size = 3\n",
        "\n",
        "# 단어 임베딩\n",
        "word_ids = Input(shape=(None,),dtype='int32', name='words_input')\n",
        "word_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(word_ids)\n",
        "\n",
        "# char 임베딩\n",
        "char_ids = Input(shape=(None, max_len_char,), name='char_input')\n",
        "embed_char_out = TimeDistributed(Embedding(len(char_to_index), char_embedding_dim, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(char_ids)\n",
        "dropout = Dropout(dropout_ratio)(embed_char_out)\n",
        "\n",
        "# char 임베딩에 대해서는 Conv1D 수행\n",
        "conv1d_out = TimeDistributed(Conv1D(kernel_size=kernel_size, filters=num_filters, padding='same',activation='tanh', strides=1))(dropout)\n",
        "maxpool_out=TimeDistributed(MaxPooling1D(max_len_char))(conv1d_out)\n",
        "char_embeddings = TimeDistributed(Flatten())(maxpool_out)\n",
        "char_embeddings = Dropout(dropout_ratio)(char_embeddings)\n",
        "\n",
        "# char 임베딩을 Conv1D 수행한 뒤에 단어 임베딩과 연결\n",
        "output = concatenate([word_embeddings, char_embeddings])\n",
        "\n",
        "# 연결한 벡터를 가지고 문장의 길이만큼 LSTM을 수행\n",
        "output = Bidirectional(LSTM(hidden_units, return_sequences=True, dropout=dropout_ratio))(output)\n",
        "\n",
        "# 출력층\n",
        "output = TimeDistributed(Dense(tag_size, activation='relu'))(output)\n",
        "\n",
        "base = Model(inputs = [word_ids, char_ids], outputs = [output])\n",
        "model = CRFModel(base, tag_size)\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.001), metrics = 'accuracy')\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('bilstm_cnn_crf/cp.ckpt', monitor='val_decode_sequence_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=True)"
      ],
      "metadata": {
        "id": "w-dNOxaJbnVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit([X_train, X_char_train], y_train_int, batch_size=128, epochs=15, validation_split=0.1, callbacks=[mc, es])"
      ],
      "metadata": {
        "id": "X5o5zvpycPip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('bilstm_cnn_crf/cp.ckpt')\n",
        "\n",
        "i = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.\n",
        "# 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
        "y_predicted = model.predict([np.array([X_test[i]]), np.array([X_char_test[i]])])[0]\n",
        "labels = np.argmax(y_test[i], -1) # 원-핫 벡터를 정수 인코딩으로 변경.\n",
        "\n",
        "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
        "print(35 * \"-\")\n",
        "\n",
        "for word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\n",
        "    if word != 0: # PAD값은 제외함.\n",
        "        print(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_ner[tag], index_to_ner[pred]))"
      ],
      "metadata": {
        "id": "ug7Xlz5pcTr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted = model.predict([X_test, X_char_test])[0]\n",
        "pred_tags = sequences_to_tag_for_crf(y_predicted)\n",
        "test_tags = sequences_to_tag(y_test)\n",
        "\n",
        "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n",
        "print(classification_report(test_tags, pred_tags))"
      ],
      "metadata": {
        "id": "F4HAE_Yfcjty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "문자 임베딩(BiLSTM) + CRF"
      ],
      "metadata": {
        "id": "wH8S24cqc4_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "char_embedding_dim = 64\n",
        "dropout_ratio = 0.3\n",
        "hidden_units = 64\n",
        "\n",
        "# 단어 임베딩\n",
        "word_ids = Input(batch_shape=(None, None), dtype='int32', name='word_input')\n",
        "word_embeddings = Embedding(input_dim=vocab_size,\n",
        "                                        output_dim=embedding_dim,\n",
        "                                        name='word_embedding')(word_ids)\n",
        "\n",
        "# char 임베딩\n",
        "char_ids = Input(batch_shape=(None, None, None), dtype='int32', name='char_input')\n",
        "char_embeddings = Embedding(input_dim=(len(char_to_index)),\n",
        "                                        output_dim=char_embedding_dim,\n",
        "                                        embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5),\n",
        "                                        name='char_embedding')(char_ids)\n",
        "\n",
        "# char 임베딩을 BiLSTM을 통과 시켜 단어 벡터를 얻고 단어 임베딩과 연결\n",
        "char_embeddings = TimeDistributed(Bidirectional(LSTM(hidden_units)))(char_embeddings)\n",
        "output = concatenate([word_embeddings, char_embeddings])\n",
        "\n",
        "# 연결한 벡터를 가지고 문장의 길이만큼 LSTM을 수행\n",
        "output = Dropout(dropout_ratio)(output)\n",
        "output = Bidirectional(LSTM(units=hidden_units, return_sequences=True))(output)\n",
        "\n",
        "# 출력층\n",
        "output = TimeDistributed(Dense(tag_size, activation='relu'))(output)\n",
        "\n",
        "base = Model(inputs=[word_ids, char_ids], outputs=[output])\n",
        "model = CRFModel(base, tag_size)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001), metrics='accuracy')\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('bilstm_bilstm_crf/cp.ckpt', monitor='val_decode_sequence_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "history = model.fit([X_train, X_char_train], y_train_int, batch_size=128, epochs=15, validation_split=0.1, callbacks=[mc, es])"
      ],
      "metadata": {
        "id": "x0eVOXCRcn52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('bilstm_bilstm_crf/cp.ckpt')\n",
        "\n",
        "i = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.\n",
        "# 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
        "y_predicted = model.predict([np.array([X_test[i]]), np.array([X_char_test[i]])])[0]\n",
        "labels = np.argmax(y_test[i], -1) # 원-핫 벡터를 정수 인코딩으로 변경.\n",
        "\n",
        "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
        "print(35 * \"-\")\n",
        "\n",
        "for word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\n",
        "    if word != 0: # PAD값은 제외함.\n",
        "        print(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_ner[tag], index_to_ner[pred]))"
      ],
      "metadata": {
        "id": "A8r5OMwTdQGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted = model.predict([X_test, X_char_test])[0]\n",
        "pred_tags = sequences_to_tag_for_crf(y_predicted)\n",
        "test_tags = sequences_to_tag(y_test)\n",
        "\n",
        "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n",
        "print(classification_report(test_tags, pred_tags))"
      ],
      "metadata": {
        "id": "7pyahbOgdS3-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}