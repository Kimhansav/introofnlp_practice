{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kimhansav/introofnlp_practice/blob/main/_14_02(Word_Level_%EB%B2%88%EC%97%AD%EA%B8%B0_%EB%A7%8C%EB%93%A4%EA%B8%B0(Neural_Machine_Translation_(seq2seq)_Tutorial))practice_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo19o6MWCzGr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import unicodedata\n",
        "import urllib3\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "def download_zip(url, output_path):\n",
        "    response = requests.get(url, headers=headers, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        with open(output_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        print(f\"ZIP file downloaded to {output_path}\")\n",
        "    else:\n",
        "        print(f\"Failed to download. HTTP Response Code: {response.status_code}\")\n",
        "\n",
        "url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
        "output_path = \"fra-eng.zip\"\n",
        "download_zip(url, output_path)\n",
        "\n",
        "path = os.getcwd()\n",
        "zipfilename = os.path.join(path, output_path)\n",
        "\n",
        "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klYfSEG5DAW5",
        "outputId": "b3a03103-b98b-4f6b-b550-201b1378c7ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded to fra-eng.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 33000"
      ],
      "metadata": {
        "id": "z_HnAgAODC63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_ascii(s):\n",
        "  #프랑스어 악센트(accent) 삭제\n",
        "  #예시: 'déjà diné' -> deja dine\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(sent):\n",
        "  #악센트 제거 함수 호출\n",
        "  sent = to_ascii(sent.lower())\n",
        "\n",
        "  #단어와 구두점 사이에 공백 추가.\n",
        "  # ex) \"I am a student.\" => \"I am a student .\"\n",
        "  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
        "\n",
        "  #(a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환\n",
        "  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "\n",
        "  #다수 개의 공백을 하나의 공백으로 치환\n",
        "  sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "  return sent"
      ],
      "metadata": {
        "id": "3XbaBwYLDJqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#전처리 테스트\n",
        "en_sent = u\"Have you had dinner?\"\n",
        "fr_sent = u\"Avez-vous déjà diné?\"\n",
        "\n",
        "print('전처리 전 영어 문장 :', en_sent)\n",
        "print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n",
        "print('전처리 전 프랑스어 문장 :', fr_sent)\n",
        "print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ta9tVjm2Es8a",
        "outputId": "b95a8486-84a2-407d-ebd1-03f875e67765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 전 영어 문장 : Have you had dinner?\n",
            "전처리 후 영어 문장 : have you had dinner ?\n",
            "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
            "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_preprocessed_data():\n",
        "  encoder_input, decoder_input, decoder_target = [], [], []\n",
        "\n",
        "  with open(\"fra.txt\", \"r\") as lines:\n",
        "    for i, line in enumerate(lines):\n",
        "      #source 데이터와 target 데이터 분리\n",
        "      src_line, tar_line, _ = line.strip().split('\\t')\n",
        "\n",
        "      #source 데이터 전처리\n",
        "      src_line = [w for w in preprocess_sentence(src_line).split()]\n",
        "\n",
        "      #target 데이터 전처리\n",
        "      tar_line = preprocess_sentence(tar_line)\n",
        "      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
        "      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
        "\n",
        "      encoder_input.append(src_line)\n",
        "      decoder_input.append(tar_line_in)\n",
        "      decoder_target.append(tar_line_out)\n",
        "\n",
        "      if i == num_samples - 1:\n",
        "        break\n",
        "  return encoder_input, decoder_input, decoder_target"
      ],
      "metadata": {
        "id": "jKuo_x8zUIxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
        "print('인코더의 입력 :',sents_en_in[:5])\n",
        "print('디코더의 입력 :',sents_fra_in[:5])\n",
        "print('디코더의 레이블 :',sents_fra_out[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVNojtEMWPwZ",
        "outputId": "755ecc6a-c12f-45b6-c93f-1d86acc10434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
            "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
            "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = Tokenizer(filters = \"\", lower = False)\n",
        "tokenizer_en.fit_on_texts(sents_en_in)\n",
        "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
        "encoder_input = pad_sequences(encoder_input, padding = 'post')\n",
        "\n",
        "tokenizer_fra = Tokenizer(filters = \"\", lower = False)\n",
        "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
        "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
        "\n",
        "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
        "decoder_input = pad_sequences(decoder_input, padding = \"post\")\n",
        "\n",
        "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n",
        "decoder_target = pad_sequences(decoder_target, padding = \"post\")"
      ],
      "metadata": {
        "id": "Op0jADspWWZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
        "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
        "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu1lw_xlZoJK",
        "outputId": "9ae27faf-7ac4-4c7d-8fe1-21d0b0c3dbaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코더의 입력의 크기(shape) : (33000, 7)\n",
            "디코더의 입력의 크기(shape) : (33000, 16)\n",
            "디코더의 레이블의 크기(shape) : (33000, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
        "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
        "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6flR_Bd4Zp-W",
        "outputId": "243d8c88-9925-41c8-f6ca-e78528dde5f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 단어 집합의 크기 : 4487, 프랑스어 단어 집합의 크기 : 7883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_to_index = tokenizer_en.word_index\n",
        "index_to_src = tokenizer_en.index_word\n",
        "tar_to_index = tokenizer_fra.word_index\n",
        "index_to_tar = tokenizer_fra.index_word"
      ],
      "metadata": {
        "id": "owwEf62qZ7fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "print('랜덤 시퀀스 :',indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih50SI7QaIbt",
        "outputId": "29f47f31-e43f-431f-c655-69946f0505dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "랜덤 시퀀스 : [20715  4613  5630 ...  3208 11239 20067]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]"
      ],
      "metadata": {
        "id": "qmOUnkNNaS0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input[30997]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEQ3B1RZab1Y",
        "outputId": "7b0e951a-aea0-444e-860b-1b7518dc1921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  29, 3373,  134,    1,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input[30997]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkvYoq6KbxY6",
        "outputId": "8e5cf54f-e6f0-47a6-87f1-59d984ce57ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2,   12,   16,   93,  314, 5433,    1,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_target[30997]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUyU5RAzbzhb",
        "outputId": "9af095d2-8088-4ee0-82d3-3de5586ee252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  12,   16,   93,  314, 5433,    1,    3,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_of_val = int(33000 * 0.1)\n",
        "print('검증 데이터의 개수 :',n_of_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no1EWEcib2nw",
        "outputId": "4a2d8bcd-f660-4a11-c2f5-7bf3c621cd13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검증 데이터의 개수 : 3300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]"
      ],
      "metadata": {
        "id": "N2SdgrnMcCmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
        "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
        "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
        "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
        "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
        "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMXDIwD9cZjH",
        "outputId": "b206d2cf-2f2f-4de1-fee9-ad385f5c3021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 source 데이터의 크기 : (29700, 7)\n",
            "훈련 target 데이터의 크기 : (29700, 16)\n",
            "훈련 target 레이블의 크기 : (29700, 16)\n",
            "테스트 source 데이터의 크기 : (3300, 7)\n",
            "테스트 target 데이터의 크기 : (3300, 16)\n",
            "테스트 target 레이블의 크기 : (3300, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "ziW7RPhvcbt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 64\n",
        "hidden_units = 64"
      ],
      "metadata": {
        "id": "kmtoUamIcjeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#인코더\n",
        "encoder_inputs = Input(shape = (None,))\n",
        "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) #임베딩층\n",
        "enc_masking = Masking(mask_value = 0.0)(enc_emb) #패딩 0은 연산에서 제외\n",
        "encoder_lstm = LSTM(hidden_units, return_state = True) #상태값 리턴을 위해 return_states는 True\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) #은닉 상태와 셀 상태를 리턴\n",
        "encoder_states = [state_h, state_c] #인코더의 은닉 상태와 셀 상태를 저장"
      ],
      "metadata": {
        "id": "01IhzNH4cma0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#디코더\n",
        "decoder_inputs = Input(shape = (None,))\n",
        "dec_emb_layer = Embedding(tar_vocab_size, hidden_units) #임베딩층\n",
        "dec_emb = dec_emb_layer(decoder_inputs) #패딩 0은 연산에서 제외\n",
        "dec_masking = Masking(mask_value = 0.0)(dec_emb)\n",
        "\n",
        "#상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
        "decoder_lstm = LSTM(hidden_units, return_sequences = True, return_state = True)\n",
        "\n",
        "#인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_masking, initial_state = encoder_states)\n",
        "\n",
        "#모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
        "decoder_dense = Dense(tar_vocab_size, activation = 'softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "#모델의 입력과 출력을 정의\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['acc'])"
      ],
      "metadata": {
        "id": "dd6FkmnDdHcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test), batch_size = 128, epochs = 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzlmfmpexncp",
        "outputId": "19fc96c3-c659-4eb6-cf81-42d6fd2d5577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "233/233 [==============================] - 33s 85ms/step - loss: 3.3637 - acc: 0.6176 - val_loss: 2.0564 - val_acc: 0.6213\n",
            "Epoch 2/50\n",
            "233/233 [==============================] - 8s 35ms/step - loss: 1.8875 - acc: 0.6467 - val_loss: 1.7769 - val_acc: 0.7079\n",
            "Epoch 3/50\n",
            "233/233 [==============================] - 7s 30ms/step - loss: 1.6793 - acc: 0.7382 - val_loss: 1.6124 - val_acc: 0.7515\n",
            "Epoch 4/50\n",
            "233/233 [==============================] - 7s 31ms/step - loss: 1.5463 - acc: 0.7547 - val_loss: 1.5065 - val_acc: 0.7615\n",
            "Epoch 5/50\n",
            "233/233 [==============================] - 7s 31ms/step - loss: 1.4408 - acc: 0.7641 - val_loss: 1.4212 - val_acc: 0.7668\n",
            "Epoch 6/50\n",
            "233/233 [==============================] - 7s 32ms/step - loss: 1.3580 - acc: 0.7752 - val_loss: 1.3535 - val_acc: 0.7798\n",
            "Epoch 7/50\n",
            "233/233 [==============================] - 7s 30ms/step - loss: 1.2891 - acc: 0.7855 - val_loss: 1.2930 - val_acc: 0.7905\n",
            "Epoch 8/50\n",
            "233/233 [==============================] - 7s 30ms/step - loss: 1.2222 - acc: 0.7999 - val_loss: 1.2376 - val_acc: 0.8019\n",
            "Epoch 9/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 1.1645 - acc: 0.8085 - val_loss: 1.1944 - val_acc: 0.8087\n",
            "Epoch 10/50\n",
            "233/233 [==============================] - 7s 31ms/step - loss: 1.1152 - acc: 0.8157 - val_loss: 1.1556 - val_acc: 0.8145\n",
            "Epoch 11/50\n",
            "233/233 [==============================] - 6s 26ms/step - loss: 1.0705 - acc: 0.8219 - val_loss: 1.1218 - val_acc: 0.8195\n",
            "Epoch 12/50\n",
            "233/233 [==============================] - 7s 29ms/step - loss: 1.0288 - acc: 0.8279 - val_loss: 1.0909 - val_acc: 0.8243\n",
            "Epoch 13/50\n",
            "233/233 [==============================] - 6s 26ms/step - loss: 0.9897 - acc: 0.8331 - val_loss: 1.0630 - val_acc: 0.8284\n",
            "Epoch 14/50\n",
            "233/233 [==============================] - 7s 30ms/step - loss: 0.9536 - acc: 0.8375 - val_loss: 1.0394 - val_acc: 0.8315\n",
            "Epoch 15/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.9204 - acc: 0.8413 - val_loss: 1.0150 - val_acc: 0.8339\n",
            "Epoch 16/50\n",
            "233/233 [==============================] - 7s 30ms/step - loss: 0.8884 - acc: 0.8444 - val_loss: 0.9948 - val_acc: 0.8369\n",
            "Epoch 17/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.8586 - acc: 0.8473 - val_loss: 0.9774 - val_acc: 0.8373\n",
            "Epoch 18/50\n",
            "233/233 [==============================] - 7s 31ms/step - loss: 0.8315 - acc: 0.8502 - val_loss: 0.9568 - val_acc: 0.8401\n",
            "Epoch 19/50\n",
            "233/233 [==============================] - 7s 30ms/step - loss: 0.8039 - acc: 0.8533 - val_loss: 0.9405 - val_acc: 0.8416\n",
            "Epoch 20/50\n",
            "233/233 [==============================] - 6s 28ms/step - loss: 0.7790 - acc: 0.8558 - val_loss: 0.9260 - val_acc: 0.8436\n",
            "Epoch 21/50\n",
            "233/233 [==============================] - 7s 29ms/step - loss: 0.7558 - acc: 0.8581 - val_loss: 0.9128 - val_acc: 0.8456\n",
            "Epoch 22/50\n",
            "233/233 [==============================] - 6s 26ms/step - loss: 0.7331 - acc: 0.8607 - val_loss: 0.8995 - val_acc: 0.8472\n",
            "Epoch 23/50\n",
            "233/233 [==============================] - 7s 30ms/step - loss: 0.7112 - acc: 0.8630 - val_loss: 0.8895 - val_acc: 0.8474\n",
            "Epoch 24/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.6913 - acc: 0.8651 - val_loss: 0.8778 - val_acc: 0.8487\n",
            "Epoch 25/50\n",
            "233/233 [==============================] - 7s 29ms/step - loss: 0.6713 - acc: 0.8676 - val_loss: 0.8685 - val_acc: 0.8496\n",
            "Epoch 26/50\n",
            "233/233 [==============================] - 6s 26ms/step - loss: 0.6529 - acc: 0.8693 - val_loss: 0.8604 - val_acc: 0.8506\n",
            "Epoch 27/50\n",
            "233/233 [==============================] - 7s 29ms/step - loss: 0.6355 - acc: 0.8713 - val_loss: 0.8511 - val_acc: 0.8507\n",
            "Epoch 28/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.6168 - acc: 0.8735 - val_loss: 0.8439 - val_acc: 0.8527\n",
            "Epoch 29/50\n",
            "233/233 [==============================] - 7s 30ms/step - loss: 0.6011 - acc: 0.8754 - val_loss: 0.8352 - val_acc: 0.8532\n",
            "Epoch 30/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.5835 - acc: 0.8775 - val_loss: 0.8277 - val_acc: 0.8542\n",
            "Epoch 31/50\n",
            "233/233 [==============================] - 7s 31ms/step - loss: 0.5675 - acc: 0.8796 - val_loss: 0.8214 - val_acc: 0.8553\n",
            "Epoch 32/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.5525 - acc: 0.8818 - val_loss: 0.8145 - val_acc: 0.8563\n",
            "Epoch 33/50\n",
            "233/233 [==============================] - 7s 30ms/step - loss: 0.5380 - acc: 0.8835 - val_loss: 0.8113 - val_acc: 0.8563\n",
            "Epoch 34/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.5235 - acc: 0.8856 - val_loss: 0.8050 - val_acc: 0.8568\n",
            "Epoch 35/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.5102 - acc: 0.8874 - val_loss: 0.7992 - val_acc: 0.8577\n",
            "Epoch 36/50\n",
            "233/233 [==============================] - 7s 28ms/step - loss: 0.4952 - acc: 0.8896 - val_loss: 0.7945 - val_acc: 0.8585\n",
            "Epoch 37/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.4825 - acc: 0.8916 - val_loss: 0.7900 - val_acc: 0.8591\n",
            "Epoch 38/50\n",
            "233/233 [==============================] - 7s 30ms/step - loss: 0.4707 - acc: 0.8934 - val_loss: 0.7867 - val_acc: 0.8595\n",
            "Epoch 39/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.4576 - acc: 0.8955 - val_loss: 0.7812 - val_acc: 0.8615\n",
            "Epoch 40/50\n",
            "233/233 [==============================] - 7s 29ms/step - loss: 0.4458 - acc: 0.8977 - val_loss: 0.7800 - val_acc: 0.8608\n",
            "Epoch 41/50\n",
            "233/233 [==============================] - 6s 26ms/step - loss: 0.4346 - acc: 0.8994 - val_loss: 0.7766 - val_acc: 0.8615\n",
            "Epoch 42/50\n",
            "233/233 [==============================] - 7s 30ms/step - loss: 0.4230 - acc: 0.9016 - val_loss: 0.7720 - val_acc: 0.8626\n",
            "Epoch 43/50\n",
            "233/233 [==============================] - 7s 28ms/step - loss: 0.4122 - acc: 0.9038 - val_loss: 0.7693 - val_acc: 0.8636\n",
            "Epoch 44/50\n",
            "233/233 [==============================] - 7s 30ms/step - loss: 0.4024 - acc: 0.9053 - val_loss: 0.7679 - val_acc: 0.8635\n",
            "Epoch 45/50\n",
            "233/233 [==============================] - 6s 28ms/step - loss: 0.3921 - acc: 0.9072 - val_loss: 0.7635 - val_acc: 0.8639\n",
            "Epoch 46/50\n",
            "233/233 [==============================] - 7s 30ms/step - loss: 0.3821 - acc: 0.9089 - val_loss: 0.7625 - val_acc: 0.8645\n",
            "Epoch 47/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.3732 - acc: 0.9106 - val_loss: 0.7632 - val_acc: 0.8643\n",
            "Epoch 48/50\n",
            "233/233 [==============================] - 7s 30ms/step - loss: 0.3641 - acc: 0.9124 - val_loss: 0.7585 - val_acc: 0.8656\n",
            "Epoch 49/50\n",
            "233/233 [==============================] - 7s 28ms/step - loss: 0.3554 - acc: 0.9143 - val_loss: 0.7592 - val_acc: 0.8655\n",
            "Epoch 50/50\n",
            "233/233 [==============================] - 6s 27ms/step - loss: 0.3474 - acc: 0.9157 - val_loss: 0.7585 - val_acc: 0.8661\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b91309294e0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#인코더\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "#디코더 설계 시작\n",
        "#이전 시점의 상태를 보관할 텐서\n",
        "decoder_state_input_h = Input(shape = (hidden_units,))\n",
        "decoder_state_input_c = Input(shape = (hidden_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "#훈련 때 사용했던 임베딩 층을 재사용\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "#다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state = decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "#모든 시점에 대해서 단어 예측\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "#수정된 디코더\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2\n",
        ")"
      ],
      "metadata": {
        "id": "dzCDGNyDx2pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "  #입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  #<SOS>에 해당하는 정수 생성\n",
        "  target_seq = np.zeros((1,1))\n",
        "  target_seq[0,0] = tar_to_index['<sos>']\n",
        "\n",
        "  stop_condition = True\n",
        "  decoded_sentence = ''\n",
        "\n",
        "  #stop_condition이 True가 될 때까지 루프 반복\n",
        "  #구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
        "  while not stop_condition:\n",
        "    #이전 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "    #예측 결과를 단어로 변환\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = index_to_tar[sampled_token_index]\n",
        "\n",
        "    #현재 시점의 예측 단어를 예측 문장에 추가\n",
        "    decoded_sentence += ' '+sampled_char\n",
        "\n",
        "    #<eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
        "    if (sampled_char == '<eos>' or len(decoded_sentence) > 50):\n",
        "      stop_condition = True\n",
        "\n",
        "    #현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0,0] = sampled_token_index\n",
        "\n",
        "    #현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "    states_value = [h, c]\n",
        "\n",
        "  return decoded_sentence"
      ],
      "metadata": {
        "id": "OUNTPLoqzi62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq_to_src(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if (encoded_word != 0):\n",
        "      sentence = sentence + index_to_src[encoded_word] + ' '\n",
        "  return sentence\n",
        "\n",
        "#번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq_to_tar(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
        "      sentence = sentence + index_to_tar[encoded_word] + ' '\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "yKDntWAg6hVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "  input_seq = encoder_input_train[seq_index : seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
        "  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
        "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02erdXbg76i8",
        "outputId": "c38b9e28-324d-4ed6-86c9-c6d75e5f64d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "입력문장 : the music stopped . \n",
            "정답문장 : la musique s est arretee . \n",
            "번역문장 : \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "입력문장 : what is your name ? \n",
            "정답문장 : comment est ce que tu t appelles ? \n",
            "번역문장 : \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "입력문장 : give me that . \n",
            "정답문장 : donnez moi ca . \n",
            "번역문장 : \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "입력문장 : rake the leaves . \n",
            "정답문장 : ratissez les feuilles . \n",
            "번역문장 : \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "입력문장 : can i pay later ? \n",
            "정답문장 : puis je payer plus tard ? \n",
            "번역문장 : \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "  input_seq = encoder_input_test[seq_index : seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
        "  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
        "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpJhFJHz8HSl",
        "outputId": "a0fd85cd-3ac6-49ad-8290-25e4f827671f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 17ms/step\n",
            "입력문장 : he cried for joy . \n",
            "정답문장 : il a pleure de joie . \n",
            "번역문장 : \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "입력문장 : thank you . \n",
            "정답문장 : merci . \n",
            "번역문장 : \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "입력문장 : take it easy ! \n",
            "정답문장 : on se calme ! \n",
            "번역문장 : \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "입력문장 : is her story true ? \n",
            "정답문장 : son histoire est elle vraie ? \n",
            "번역문장 : \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "입력문장 : i sat at the bar . \n",
            "정답문장 : je me suis assis au bar . \n",
            "번역문장 : \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HbH35lqz-DC1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}